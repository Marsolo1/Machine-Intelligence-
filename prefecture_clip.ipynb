{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fumiyatanaka/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"fummicc1/hiyoshi-street-clip\")\n",
    "processor = AutoProcessor.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "\n",
    "# model = AutoModelForZeroShotImageClassification.from_pretrained(\"fummicc1/hiyoshi-street-clip\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"geolocal/StreetCLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"CLIPProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='geolocal/StreetCLIP', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hiyoshi', 'Hiyoshihonchol', 'Minowacho']\n"
     ]
    }
   ],
   "source": [
    "town_name = \"Minowacho\"\n",
    "predict_town = True\n",
    "town = pathlib.Path(f\"data/addrs/{town_name}\")\n",
    "\n",
    "postal_code_maps = {\n",
    "    'Minowacho': '223-0051',\n",
    "    'HiyoshiHoncho': '223-0062',\n",
    "    'Hiyoshi': '223-0061',\n",
    "}\n",
    "\n",
    "hiyoshi_maps = {\n",
    "    \"1-chome\": \"〒223-0061 神奈川県横浜市港北区日吉1丁目\",\n",
    "    \"2-chome\": \"〒223-0061 神奈川県横浜市港北区日吉2丁目\",\n",
    "    \"3-chome\": \"〒223-0061 神奈川県横浜市港北区日吉3丁目\",\n",
    "    \"4-chome\": \"〒223-0061 神奈川県横浜市港北区日吉4丁目\",\n",
    "    \"5-chome\": \"〒223-0061 神奈川県横浜市港北区日吉5丁目\",\n",
    "    \"6-chome\": \"〒223-0061 神奈川県横浜市港北区日吉6丁目\",\n",
    "    \"7-chome\": \"〒223-0061 神奈川県横浜市港北区日吉7丁目\",\n",
    "}\n",
    "label_maps = {\n",
    "    'Hiyoshi': 'Hiyoshi',\n",
    "    'Hiyoshihoncho': 'Hiyoshihonchol',\n",
    "    'Minowacho': 'Minowacho',\n",
    "}\n",
    "labels = []\n",
    "\n",
    "if predict_town:\n",
    "    labels = list(label_maps.values())\n",
    "else:\n",
    "    for folder in sorted(list(town.iterdir())):\n",
    "        name = folder.name\n",
    "        # if maps.get(name) is not None:\n",
    "            # labels.append(maps[name])\n",
    "        if name not in hiyoshi_maps:\n",
    "            continue\n",
    "        labels.append(name)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(labels: List[str], img: torch.Tensor):\n",
    "    inputs = processor(\n",
    "        text=labels,\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    print(\"probs:\", probs)\n",
    "    index = torch.argmax(probs, dim=1).item()\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: 35.54571017-139.64016186.png\n",
      "probs: tensor([[0.0961, 0.8464, 0.0576]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihonchol\n",
      "img: 35.54520193-139.64016186.png\n",
      "probs: tensor([[0.5062, 0.4603, 0.0335]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54520193-139.64168843.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = pathlib.Path(f\"data/addrs/{town_name}/3-chome\")\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for img_path in path.iterdir():\n",
    "    img = Image.open(img_path.as_posix()).convert(\"RGB\").resize((336, 336))\n",
    "    print(\"img:\", img_path.name)\n",
    "    ret = predict(labels, img=img)\n",
    "    print(\"result\", ret)\n",
    "    if ret == label_maps[town_name]:\n",
    "        acc += 1\n",
    "acc /= len(list(path.iterdir()))\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACCURACY\n",
    "\n",
    "|pretrained|finetuning|\n",
    "|---|---|\n",
    "|0.02631578947368421||\n",
    "\n",
    "## F1\n",
    "\n",
    "|pretrained|finetuning|\n",
    "|---|---|\n",
    "|||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
