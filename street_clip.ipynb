{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"geolocal/StreetCLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"CLIPProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='geolocal/StreetCLIP', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['日吉', '日吉本町', '箕輪町']\n"
     ]
    }
   ],
   "source": [
    "town_name = \"Minowacho\"\n",
    "predict_town = True\n",
    "town = pathlib.Path(f\"data/addrs/{town_name}\")\n",
    "\n",
    "maps = {\n",
    "    \"1-chome\": \"〒223-0061 神奈川県横浜市港北区日吉1丁目\",\n",
    "    \"2-chome\": \"〒223-0061 神奈川県横浜市港北区日吉2丁目\",\n",
    "    \"3-chome\": \"〒223-0061 神奈川県横浜市港北区日吉3丁目\",\n",
    "    \"4-chome\": \"〒223-0061 神奈川県横浜市港北区日吉4丁目\",\n",
    "    \"5-chome\": \"〒223-0061 神奈川県横浜市港北区日吉5丁目\",\n",
    "    \"6-chome\": \"〒223-0061 神奈川県横浜市港北区日吉6丁目\",\n",
    "    \"7-chome\": \"〒223-0061 神奈川県横浜市港北区日吉7丁目\",\n",
    "}\n",
    "\n",
    "labels = []\n",
    "\n",
    "if predict_town:\n",
    "    labels = [\n",
    "        \"Hiyoshi\",\n",
    "        \"Hiyoshihoncho\",\n",
    "        \"Minowacho\"\n",
    "    ]\n",
    "else:\n",
    "    for folder in sorted(list(town.iterdir())):\n",
    "        name = folder.name\n",
    "        # if maps.get(name) is not None:\n",
    "            # labels.append(maps[name])\n",
    "        if name not in maps:\n",
    "            continue\n",
    "        labels.append(name)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(labels: List[str], img: torch.Tensor):\n",
    "    inputs = processor(\n",
    "        text=labels,\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    print(\"probs:\", probs)\n",
    "    index = torch.argmax(probs, dim=1).item()\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: 35.54571017-139.64016186.png\n",
      "probs: tensor([[0.0653, 0.6180, 0.3167]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54520193-139.64016186.png\n",
      "probs: tensor([[0.0173, 0.6211, 0.3616]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54520193-139.64168843.png\n",
      "probs: tensor([[0.0232, 0.7503, 0.2266]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54774315-139.64321500.png\n",
      "probs: tensor([[0.0323, 0.6986, 0.2691]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54723490-139.64321500.png\n",
      "probs: tensor([[0.0184, 0.3875, 0.5941]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54571017-139.64168843.png\n",
      "probs: tensor([[0.0302, 0.5802, 0.3896]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54977612-139.64474157.png\n",
      "probs: tensor([[0.0221, 0.6759, 0.3020]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54977612-139.64525043.png\n",
      "probs: tensor([[0.0204, 0.5316, 0.4480]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54825139-139.64168843.png\n",
      "probs: tensor([[0.0546, 0.1661, 0.7793]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54418544-139.64067071.png\n",
      "probs: tensor([[0.0168, 0.8014, 0.1818]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54723490-139.64067071.png\n",
      "probs: tensor([[0.0437, 0.7111, 0.2452]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54621841-139.64117957.png\n",
      "probs: tensor([[0.0350, 0.5078, 0.4572]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54723490-139.64219729.png\n",
      "probs: tensor([[0.0235, 0.5034, 0.4731]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54774315-139.64270614.png\n",
      "probs: tensor([[0.0830, 0.6568, 0.2602]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54723490-139.64270614.png\n",
      "probs: tensor([[0.0159, 0.5369, 0.4472]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54774315-139.64219729.png\n",
      "probs: tensor([[0.0222, 0.2383, 0.7395]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54774315-139.64067071.png\n",
      "probs: tensor([[0.0760, 0.3625, 0.5616]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54571017-139.63965300.png\n",
      "probs: tensor([[0.0259, 0.6114, 0.3627]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54520193-139.63965300.png\n",
      "probs: tensor([[0.0270, 0.5855, 0.3875]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54621841-139.64016186.png\n",
      "probs: tensor([[0.0290, 0.7859, 0.1851]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54469368-139.64067071.png\n",
      "probs: tensor([[0.0413, 0.6518, 0.3068]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54875963-139.64474157.png\n",
      "probs: tensor([[0.0139, 0.9023, 0.0838]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.55079261-139.64219729.png\n",
      "probs: tensor([[0.0338, 0.2417, 0.7245]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.55079261-139.64270614.png\n",
      "probs: tensor([[0.1379, 0.2443, 0.6178]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54774315-139.64423271.png\n",
      "probs: tensor([[0.0180, 0.7565, 0.2255]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54875963-139.64372386.png\n",
      "probs: tensor([[0.0751, 0.7083, 0.2166]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54825139-139.64372386.png\n",
      "probs: tensor([[0.0263, 0.8093, 0.1645]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54926788-139.64474157.png\n",
      "probs: tensor([[0.0190, 0.5583, 0.4228]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54621841-139.64168843.png\n",
      "probs: tensor([[0.0412, 0.5477, 0.4111]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54672666-139.64270614.png\n",
      "probs: tensor([[0.0326, 0.4575, 0.5099]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54672666-139.64219729.png\n",
      "probs: tensor([[0.0617, 0.4843, 0.4540]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54672666-139.64067071.png\n",
      "probs: tensor([[0.0471, 0.5524, 0.4005]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54926788-139.64372386.png\n",
      "probs: tensor([[0.0609, 0.5159, 0.4232]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.55028437-139.64474157.png\n",
      "probs: tensor([[0.0585, 0.6769, 0.2646]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54926788-139.64525043.png\n",
      "probs: tensor([[0.0120, 0.4313, 0.5567]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54672666-139.64321500.png\n",
      "probs: tensor([[0.0145, 0.7328, 0.2527]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54520193-139.64117957.png\n",
      "probs: tensor([[0.0294, 0.4521, 0.5184]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54571017-139.64117957.png\n",
      "probs: tensor([[0.0190, 0.5682, 0.4128]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.55079261-139.64321500.png\n",
      "probs: tensor([[0.0456, 0.3042, 0.6503]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54977612-139.64423271.png\n",
      "probs: tensor([[0.0591, 0.5485, 0.3924]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54926788-139.64321500.png\n",
      "probs: tensor([[0.0504, 0.5622, 0.3874]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54825139-139.64321500.png\n",
      "probs: tensor([[0.0370, 0.8073, 0.1557]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54875963-139.64321500.png\n",
      "probs: tensor([[0.0451, 0.5631, 0.3918]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54520193-139.64067071.png\n",
      "probs: tensor([[0.0347, 0.5159, 0.4494]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.55028437-139.64219729.png\n",
      "probs: tensor([[0.0822, 0.2329, 0.6849]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54571017-139.64270614.png\n",
      "probs: tensor([[0.0120, 0.6055, 0.3826]], grad_fn=<SoftmaxBackward0>)\n",
      "result 日吉本町\n",
      "img: 35.54571017-139.64219729.png\n",
      "probs: tensor([[0.0421, 0.4252, 0.5327]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.55028437-139.64270614.png\n",
      "probs: tensor([[0.0936, 0.0382, 0.8682]], grad_fn=<SoftmaxBackward0>)\n",
      "result 箕輪町\n",
      "img: 35.54571017-139.64067071.png\n",
      "probs: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 11\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# if err_img == img:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m#     os.remove(img_path)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#     print(\"removed:\", img_path.as_posix())\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m#     continue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mimg:\u001b[39m\u001b[39m\"\u001b[39m, img_path\u001b[39m.\u001b[39mname)\n\u001b[0;32m---> 11\u001b[0m ret \u001b[39m=\u001b[39m predict(labels, img\u001b[39m=\u001b[39;49mimg)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m, ret)\n",
      "Cell \u001b[0;32mIn[126], line 11\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(labels, img)\u001b[0m\n\u001b[1;32m      9\u001b[0m logits_per_image \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlogits_per_image \u001b[39m# this is the image-text similarity score\u001b[39;00m\n\u001b[1;32m     10\u001b[0m probs \u001b[39m=\u001b[39m logits_per_image\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[39mprint\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprobs:\u001b[39;49m\u001b[39m\"\u001b[39;49m, probs)\n\u001b[1;32m     12\u001b[0m index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(probs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m labels[index]\n",
      "File \u001b[0;32m~/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/torch/_tensor.py:426\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    423\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_tensor_str\u001b[39m.\u001b[39;49m_str(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/torch/_tensor_str.py:636\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    635\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 636\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39;49m, tensor_contents\u001b[39m=\u001b[39;49mtensor_contents)\n",
      "File \u001b[0;32m~/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/torch/_tensor_str.py:567\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    565\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    566\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39;49m, indent)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    570\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/torch/_tensor_str.py:327\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    324\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39;49m) \u001b[39mif\u001b[39;49;00m summarize \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    328\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/Work/Univ/Spr/MachineIntelligence/Machine-Intelligence-Team-8/.venv/lib/python3.10/site-packages/torch/_tensor_str.py:131\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mceil(value):\n\u001b[1;32m    130\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mint_mode \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mint_mode:\n\u001b[1;32m    134\u001b[0m     \u001b[39m# in int_mode for floats, all numbers are integers, and we append a decimal to nonfinites\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     \u001b[39m# to indicate that the tensor is of floating type. add 1 to the len to account for this.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m         nonzero_finite_max \u001b[39m/\u001b[39m nonzero_finite_min \u001b[39m>\u001b[39m \u001b[39m1000.0\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[39mor\u001b[39;00m nonzero_finite_max \u001b[39m>\u001b[39m \u001b[39m1.0e8\u001b[39m\n\u001b[1;32m    139\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "path = pathlib.Path(f\"data/addrs/{town_name}/3-chome\")\n",
    "err_img = Image.open(pathlib.Path(\"data/error_img.png\").as_posix()).convert(\"RGB\")\n",
    "\n",
    "for img_path in path.iterdir():\n",
    "    img = Image.open(img_path.as_posix()).convert(\"RGB\")\n",
    "    # if err_img == img:\n",
    "    #     os.remove(img_path)\n",
    "    #     print(\"removed:\", img_path.as_posix())\n",
    "    #     continue\n",
    "    print(\"img:\", img_path.name)\n",
    "    ret = predict(labels, img=img)\n",
    "    print(\"result\", ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
