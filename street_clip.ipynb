{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"geolocal/StreetCLIP\")\n",
    "\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"geolocal/StreetCLIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(577, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPProcessor:\n",
       "- image_processor: CLIPImageProcessor {\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"CLIPFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"processor_class\": \"CLIPProcessor\",\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CLIPTokenizerFast(name_or_path='geolocal/StreetCLIP', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_name = \"Minowacho\"\n",
    "predict_town = True\n",
    "town = pathlib.Path(f\"data/addrs/{town_name}\")\n",
    "\n",
    "maps = {\n",
    "    \"1-chome\": \"〒223-0061 神奈川県横浜市港北区日吉1丁目\",\n",
    "    \"2-chome\": \"〒223-0061 神奈川県横浜市港北区日吉2丁目\",\n",
    "    \"3-chome\": \"〒223-0061 神奈川県横浜市港北区日吉3丁目\",\n",
    "    \"4-chome\": \"〒223-0061 神奈川県横浜市港北区日吉4丁目\",\n",
    "    \"5-chome\": \"〒223-0061 神奈川県横浜市港北区日吉5丁目\",\n",
    "    \"6-chome\": \"〒223-0061 神奈川県横浜市港北区日吉6丁目\",\n",
    "    \"7-chome\": \"〒223-0061 神奈川県横浜市港北区日吉7丁目\",\n",
    "}\n",
    "\n",
    "labels = []\n",
    "\n",
    "if predict_town:\n",
    "    labels = [\n",
    "        \"Hiyoshi\",\n",
    "        \"Hiyoshihoncho\",\n",
    "        \"Minowacho\"\n",
    "    ]\n",
    "else:\n",
    "    for folder in sorted(list(town.iterdir())):\n",
    "        name = folder.name\n",
    "        # if maps.get(name) is not None:\n",
    "            # labels.append(maps[name])\n",
    "        if name not in maps:\n",
    "            continue\n",
    "        labels.append(name)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(labels: List[str], img: torch.Tensor):\n",
    "    inputs = processor(\n",
    "        text=labels,\n",
    "        images=img,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "    print(\"probs:\", probs)\n",
    "    index = torch.argmax(probs, dim=1).item()\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: 35.54571017-139.64016186.png\n",
      "probs: tensor([[0.2562, 0.5904, 0.1534]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54520193-139.64016186.png\n",
      "probs: tensor([[0.7323, 0.2192, 0.0485]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54520193-139.64168843.png\n",
      "probs: tensor([[0.5709, 0.3554, 0.0738]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.64321500.png\n",
      "probs: tensor([[0.7317, 0.2107, 0.0576]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54723490-139.64321500.png\n",
      "probs: tensor([[0.6707, 0.2918, 0.0375]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.64168843.png\n",
      "probs: tensor([[0.8575, 0.1120, 0.0304]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54977612-139.64474157.png\n",
      "probs: tensor([[0.6889, 0.2551, 0.0561]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54977612-139.64525043.png\n",
      "probs: tensor([[0.3943, 0.5149, 0.0908]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54825139-139.64168843.png\n",
      "probs: tensor([[0.2286, 0.6833, 0.0881]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54418544-139.64067071.png\n",
      "probs: tensor([[0.7684, 0.1658, 0.0657]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54723490-139.64067071.png\n",
      "probs: tensor([[0.4567, 0.4812, 0.0621]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54621841-139.64117957.png\n",
      "probs: tensor([[0.3700, 0.5535, 0.0765]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54723490-139.64219729.png\n",
      "probs: tensor([[0.2822, 0.6146, 0.1031]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54774315-139.64270614.png\n",
      "probs: tensor([[0.2313, 0.5096, 0.2590]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54723490-139.64270614.png\n",
      "probs: tensor([[0.3198, 0.5432, 0.1370]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54774315-139.64219729.png\n",
      "probs: tensor([[0.8350, 0.1382, 0.0268]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.64067071.png\n",
      "probs: tensor([[0.6473, 0.3058, 0.0469]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.63965300.png\n",
      "probs: tensor([[0.4469, 0.4279, 0.1252]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54520193-139.63965300.png\n",
      "probs: tensor([[0.6663, 0.2755, 0.0582]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54621841-139.64016186.png\n",
      "probs: tensor([[0.7108, 0.2353, 0.0540]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54469368-139.64067071.png\n",
      "probs: tensor([[0.3651, 0.5090, 0.1259]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54875963-139.64474157.png\n",
      "probs: tensor([[0.8049, 0.1525, 0.0427]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.55079261-139.64219729.png\n",
      "probs: tensor([[0.9040, 0.0816, 0.0144]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.55079261-139.64270614.png\n",
      "probs: tensor([[0.9398, 0.0421, 0.0181]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.64423271.png\n",
      "probs: tensor([[0.3928, 0.5427, 0.0645]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54875963-139.64372386.png\n",
      "probs: tensor([[0.4203, 0.4930, 0.0867]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54825139-139.64372386.png\n",
      "probs: tensor([[0.7868, 0.1583, 0.0549]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54926788-139.64474157.png\n",
      "probs: tensor([[0.5730, 0.3468, 0.0802]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54621841-139.64168843.png\n",
      "probs: tensor([[0.3061, 0.5260, 0.1679]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54672666-139.64270614.png\n",
      "probs: tensor([[0.8178, 0.1273, 0.0549]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64219729.png\n",
      "probs: tensor([[0.7613, 0.2135, 0.0252]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64067071.png\n",
      "probs: tensor([[0.2501, 0.6938, 0.0561]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54926788-139.64372386.png\n",
      "probs: tensor([[0.6809, 0.2625, 0.0566]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.55028437-139.64474157.png\n",
      "probs: tensor([[0.3865, 0.5024, 0.1111]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54926788-139.64525043.png\n",
      "probs: tensor([[0.6153, 0.2992, 0.0855]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64321500.png\n",
      "probs: tensor([[0.5787, 0.3305, 0.0908]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54520193-139.64117957.png\n",
      "probs: tensor([[0.8491, 0.1304, 0.0205]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.64117957.png\n",
      "probs: tensor([[0.7083, 0.2389, 0.0528]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.55079261-139.64321500.png\n",
      "probs: tensor([[0.4920, 0.4514, 0.0565]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54977612-139.64423271.png\n",
      "probs: tensor([[0.5081, 0.3776, 0.1143]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54926788-139.64321500.png\n",
      "probs: tensor([[0.6875, 0.2511, 0.0613]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54825139-139.64321500.png\n",
      "probs: tensor([[0.4824, 0.3652, 0.1523]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54875963-139.64321500.png\n",
      "probs: tensor([[0.5005, 0.4027, 0.0968]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54520193-139.64067071.png\n",
      "probs: tensor([[0.6531, 0.2792, 0.0678]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.55028437-139.64219729.png\n",
      "probs: tensor([[0.8607, 0.1172, 0.0221]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.64270614.png\n",
      "probs: tensor([[0.8409, 0.1327, 0.0264]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.64219729.png\n",
      "probs: tensor([[0.2918, 0.6072, 0.1010]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.55028437-139.64270614.png\n",
      "probs: tensor([[0.7982, 0.1238, 0.0780]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54571017-139.64067071.png\n",
      "probs: tensor([[0.4703, 0.4564, 0.0733]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.64016186.png\n",
      "probs: tensor([[0.6473, 0.3058, 0.0469]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54875963-139.64270614.png\n",
      "probs: tensor([[0.8452, 0.1270, 0.0277]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54723490-139.64016186.png\n",
      "probs: tensor([[0.3409, 0.5723, 0.0868]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54926788-139.64270614.png\n",
      "probs: tensor([[0.7924, 0.1698, 0.0378]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54469368-139.64117957.png\n",
      "probs: tensor([[0.5005, 0.3827, 0.1168]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54825139-139.64270614.png\n",
      "probs: tensor([[0.4790, 0.4007, 0.1202]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54926788-139.64219729.png\n",
      "probs: tensor([[0.5812, 0.3681, 0.0507]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54418544-139.64016186.png\n",
      "probs: tensor([[0.5856, 0.3615, 0.0529]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54723490-139.64168843.png\n",
      "probs: tensor([[0.5107, 0.4281, 0.0612]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64117957.png\n",
      "probs: tensor([[0.5102, 0.4284, 0.0613]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64016186.png\n",
      "probs: tensor([[0.6232, 0.3215, 0.0553]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54825139-139.64423271.png\n",
      "probs: tensor([[0.6208, 0.2921, 0.0871]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54926788-139.64423271.png\n",
      "probs: tensor([[0.7349, 0.2219, 0.0432]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54723490-139.64372386.png\n",
      "probs: tensor([[0.6736, 0.2732, 0.0532]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54469368-139.64168843.png\n",
      "probs: tensor([[0.5029, 0.4111, 0.0860]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54418544-139.64117957.png\n",
      "probs: tensor([[0.7496, 0.2028, 0.0476]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.64372386.png\n",
      "probs: tensor([[0.5173, 0.4028, 0.0799]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54875963-139.64423271.png\n",
      "probs: tensor([[0.4281, 0.5022, 0.0697]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54621841-139.64067071.png\n",
      "probs: tensor([[0.5940, 0.3558, 0.0502]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54621841-139.64219729.png\n",
      "probs: tensor([[0.4375, 0.4742, 0.0882]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54723490-139.64117957.png\n",
      "probs: tensor([[0.5519, 0.3996, 0.0485]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54621841-139.64270614.png\n",
      "probs: tensor([[0.5542, 0.3324, 0.1134]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54672666-139.64168843.png\n",
      "probs: tensor([[0.5049, 0.4282, 0.0668]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54977612-139.64219729.png\n",
      "probs: tensor([[0.9102, 0.0719, 0.0179]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54774315-139.63965300.png\n",
      "probs: tensor([[0.4060, 0.4894, 0.1046]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshihoncho\n",
      "img: 35.54977612-139.64270614.png\n",
      "probs: tensor([[0.5702, 0.3677, 0.0622]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n",
      "img: 35.54621841-139.64321500.png\n",
      "probs: tensor([[0.7614, 0.1910, 0.0476]], grad_fn=<SoftmaxBackward0>)\n",
      "result Hiyoshi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = pathlib.Path(f\"data/addrs/{town_name}/3-chome\")\n",
    "err_img = Image.open(pathlib.Path(\"data/error_img.png\").as_posix()).convert(\"RGB\")\n",
    "\n",
    "for img_path in path.iterdir():\n",
    "    img = Image.open(img_path.as_posix()).convert(\"RGB\")\n",
    "    # if err_img == img:\n",
    "    #     os.remove(img_path)\n",
    "    #     print(\"removed:\", img_path.as_posix())\n",
    "    #     continue\n",
    "    print(\"img:\", img_path.name)\n",
    "    ret = predict(labels, img=img)\n",
    "    print(\"result\", ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
